{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --pre mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_01 =  pd.read_csv('./final_training_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Split the Final Dataset into Features and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data_01.drop('PromotionStatus', axis=1) \n",
    "y = training_data_01['PromotionStatus'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Split the Dataset into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 17)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install imbalanced-learn\n",
    "# \n",
    "X_train.shape\n",
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Experiment-1 \n",
    "    RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[ 239   50    0]\n",
      " [  48 1084   57]\n",
      " [   0   80  442]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       289\n",
      "           1       0.89      0.91      0.90      1189\n",
      "           2       0.89      0.85      0.87       522\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.87      0.86      0.87      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\"random_state\":42 ,\"n_estimators\": 100,\"max_depth\":10}\n",
    "\n",
    "model = RandomForestClassifier(**params)  \n",
    "# model.fit(X_train, y_train)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.863  0.843  0.868  0.86   0.8665]\n",
      "Mean accuracy: 0.8600999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5) \n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create dictionary of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Log the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"First Experiment\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"Random Forest Classifier\"):\n",
    "    mlflow.set_tag(\"Training Info\", \"Trained with Project final Data\")\n",
    "    # mlflow.log_params({'random_state': 42})\n",
    "    \n",
    "    mlflow.log_params(params=params)\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        'accuracy': report_dict['accuracy'],\n",
    "        'recall_class_0': report_dict['0']['recall'],\n",
    "        'recall_class_1': report_dict['1']['recall'],\n",
    "        'recall_class_2': report_dict['2']['recall'],\n",
    "        'precision_class_0':report_dict['0']['precision'],\n",
    "        'precision_class_1':report_dict['1']['precision'],\n",
    "        'precision_class_2':report_dict['2']['precision'],\n",
    "        'f1_score_macro': report_dict['macro avg']['f1-score']\n",
    "    })\n",
    "    # mlflow.log_metric('Cross-validation scores',scores)\n",
    "    mlflow.log_metric('cross_val_mean_accuracy', scores.mean())\n",
    "\n",
    "    model_info=  mlflow.sklearn.log_model(model, \"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Load our saved model as a Python Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_1 = mlflow.pyfunc.load_model(\"file:///c:/Assessments/final_project/Data_Science/mlruns/140846681390198027/6d3fc1e2dac64a0db9e272d9b3fd82cc/artifacts/Random Forest Classifier\")\n",
    "loaded_model_2 = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "# print(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Use our model to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[ 233   56    0]\n",
      " [  42 1104   43]\n",
      " [   0   80  442]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       289\n",
      "           1       0.89      0.93      0.91      1189\n",
      "           2       0.91      0.85      0.88       522\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.88      0.86      0.87      2000\n",
      "weighted avg       0.89      0.89      0.89      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_model_1.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix\\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report\\n\",classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Experiment-2 \n",
    "    Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[160 127   2]\n",
      " [  0 925 264]\n",
      " [  0 142 380]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71       289\n",
      "           1       0.77      0.78      0.78      1189\n",
      "           2       0.59      0.73      0.65       522\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.79      0.69      0.71      2000\n",
      "weighted avg       0.76      0.73      0.73      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VaibhavAgarwal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)  \n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Log different Models\n",
    "    Tract Experiments using MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\n",
    "        \"Random Forest Classifier\",\n",
    "        {\"random_state\":42, \"max_depth\":10,\"n_estimators\" :100},\n",
    "        RandomForestClassifier(random_state=42, max_depth=10,n_estimators=100),\n",
    "        (X_train_resampled,y_train_resampled),\n",
    "        (X_test,y_test)\n",
    "     ) ,\n",
    "     \n",
    "     (\n",
    "        \"Logistic Regression Model\",\n",
    "        {\"max_iter\":1000},\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        (X_train_resampled,y_train_resampled),\n",
    "        (X_test,y_test)    \n",
    "     )\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VaibhavAgarwal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "\n",
    "for model_name,params, model, train_set, test_set in models:\n",
    "    X_train = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test,y_pred, output_dict=True)\n",
    "    reports.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': {'precision': 0.8327526132404182,\n",
       "   'recall': 0.8269896193771626,\n",
       "   'f1-score': 0.8298611111111112,\n",
       "   'support': 289.0},\n",
       "  '1': {'precision': 0.8929159802306426,\n",
       "   'recall': 0.911690496215307,\n",
       "   'f1-score': 0.9022055763628797,\n",
       "   'support': 1189.0},\n",
       "  '2': {'precision': 0.8857715430861723,\n",
       "   'recall': 0.8467432950191571,\n",
       "   'f1-score': 0.8658178256611165,\n",
       "   'support': 522.0},\n",
       "  'accuracy': 0.8825,\n",
       "  'macro avg': {'precision': 0.8704800455190776,\n",
       "   'recall': 0.861807803537209,\n",
       "   'f1-score': 0.8659615043783692,\n",
       "   'support': 2000.0},\n",
       "  'weighted avg': {'precision': 0.8823576756058485,\n",
       "   'recall': 0.8825,\n",
       "   'f1-score': 0.882254598200839,\n",
       "   'support': 2000.0}},\n",
       " {'0': {'precision': 1.0,\n",
       "   'recall': 0.5536332179930796,\n",
       "   'f1-score': 0.7126948775055679,\n",
       "   'support': 289.0},\n",
       "  '1': {'precision': 0.7747068676716918,\n",
       "   'recall': 0.7779646761984861,\n",
       "   'f1-score': 0.7763323541754091,\n",
       "   'support': 1189.0},\n",
       "  '2': {'precision': 0.5882352941176471,\n",
       "   'recall': 0.7279693486590039,\n",
       "   'f1-score': 0.6506849315068494,\n",
       "   'support': 522.0},\n",
       "  'accuracy': 0.7325,\n",
       "  'macro avg': {'precision': 0.787647387263113,\n",
       "   'recall': 0.6865224142835231,\n",
       "   'f1-score': 0.7132373877292756,\n",
       "   'support': 2000.0},\n",
       "  'weighted avg': {'precision': 0.7585926445955268,\n",
       "   'recall': 0.7325,\n",
       "   'f1-score': 0.7343427614801229,\n",
       "   'support': 2000.0}}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/28 14:34:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/10/28 14:34:41 INFO mlflow.tracking._tracking_service.client: üèÉ View run Random Forest Classifier at: http://127.0.0.1:5000/#/experiments/887698436236583196/runs/53560df7e8fa4cabb6e95586a73e633c.\n",
      "2024/10/28 14:34:41 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/887698436236583196.\n",
      "2024/10/28 14:34:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/10/28 14:34:47 INFO mlflow.tracking._tracking_service.client: üèÉ View run Logistic Regression Model at: http://127.0.0.1:5000/#/experiments/887698436236583196/runs/5ff849e9827f4335b3ad0d67ee00fca1.\n",
      "2024/10/28 14:34:47 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/887698436236583196.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"Log Differetn Models\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    params = element[1]\n",
    "    model = element[2]\n",
    "    report = reports[i]\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # mlflow.set_tag(\"Training Info\", \"Trained with Project final Data\")\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "        'accuracy': report['accuracy'],\n",
    "        'recall_class_0': report['0']['recall'],\n",
    "        'recall_class_1': report['1']['recall'],\n",
    "        'recall_class_2': report['2']['recall'],\n",
    "        'precision_class_0':report['0']['precision'],\n",
    "        'precision_class_1':report['1']['precision'],\n",
    "        'precision_class_2':report['2']['precision'],\n",
    "        'f1_score_macro': report['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "\n",
    "        # mlflow.log_metric('accuracy',report['accuracy'])\n",
    "        # mlflow.log_metric('Cross-validation scores',scores)\n",
    "        # mlflow.log_metric('cross_val_mean_accuracy', scores.mean())\n",
    "\n",
    "        model_info  =  mlflow.sklearn.log_model(model, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
